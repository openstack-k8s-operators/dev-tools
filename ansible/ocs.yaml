---
- hosts: convergence_base
  gather_facts: false
  become: true
  user: root
  vars_files: vars/default.yaml

  tasks:
  - name: Prepare {{ ocs_worker_domain }} VM for use as a storage node
    when: ocs_enabled == true
    block:
    - name: Get current {{ ocs_worker_domain }} VM attached disk count
      shell: "virsh dumpxml {{ ocs_worker_domain }} | grep \"source file='/home/ocs/data/ocs_disk\" | wc -l"
      register: cur_storage_disks

    - name: Get current {{ ocs_worker_domain }} VM memory size
      shell: "virsh dominfo {{ ocs_worker_domain }} | grep \"Max memory\" | tr -d ' ' | cut -d ':' -f 2 | cut -d 'K' -f 1 | awk '{$1=$1/1024; print $1;}'"
      register: cur_storage_memory

    - name: Get current {{ ocs_worker_domain }} VM vCPU count
      shell: virsh dominfo ostest_worker_1 | grep CPU\(s\) | tr -d ' ' | cut -d ':' -f 2
      register: cur_storage_cpu

    - name: Change {{ ocs_worker_domain }} VM specs
      when: (cur_storage_disks.stdout | int) != (ocs_disks | length) or (cur_storage_memory.stdout | int) != ocp_storage_memory or (cur_storage_cpu.stdout | int) != ocp_storage_vcpu
      block:
      - name: Clear host VM memory cache
        shell: echo 3 | tee /proc/sys/vm/drop_caches

      - name: Create OCS data disks for {{ ocs_worker_domain }} VM
        shell: |
          set -e -o pipefail

          for i in {1..{{ ocs_disks | length }}}; do
              fs="{{ ocs_data_dir }}/ocs_disk_${i}"

              if [ ! -f "$fs" ]; then
                  # Create a sparse file of the correct size and populate it with an
                  # ext2 filesystem.
                  mkdir -p {{ ocs_data_dir }}
                  truncate -s {{ ocs_disk_size }}G $fs
                  mkfs.ext2 -m 0 "$fs"

                  # Make world readable
                  chown nobody.nobody "$fs"
                  chmod 0777 "$fs"
              fi
          done

      - name: Stop {{ ocs_worker_domain }} VM
        virt: 
          name: "{{ ocs_worker_domain }}"
          state: destroyed

      - name: Attach data disks to {{ ocs_worker_domain }} VM
        command: "virsh attach-disk {{ ocs_worker_domain }} --source {{ ocs_data_dir }}/ocs_disk_{{ index + 1 }} --target {{ item }} --persistent"
        register: attach_disks
        failed_when: attach_disks.stderr != "" and "already exists" not in attach_disks.stderr
        loop: "{{ ocs_disks }}"
        loop_control:
          index_var: index

      - name: Set {{ ocs_worker_domain }} VM cpu and memory specs
        block:
        - name: Set {{ ocs_worker_domain }} VM memory
          shell: |
            virsh setmaxmem {{ ocs_worker_domain }} {{ ocp_storage_memory }}M --config;
            virsh setmem {{ ocs_worker_domain }} {{ ocp_storage_memory }}M --config

        - name: Set {{ ocs_worker_domain }} VM max and current cpus
          shell: |
            virsh setvcpus {{ ocs_worker_domain }} {{ ocp_storage_vcpu }} --config --maximum;
            virsh setvcpus {{ ocs_worker_domain }} {{ ocp_storage_vcpu }} --current

      - name: Register fact that {{ ocs_worker_domain }} VM specs changed
        set_fact:
          storage_node_specs_updated: true

- hosts: localhost
  gather_facts: false
  become: true
  user: root
  vars_files: vars/default.yaml
  roles:
  - oc_local
  tasks:
  - name: Install OCS
    when: ocs_enabled == true
    block:
    - name: De-provision and reprovision associated baremetal host (if {{ocs_worker_domain }} specs changed)
      when: storage_node_specs_updated is defined and storage_node_specs_updated == true
      block:
      - name: Annotate associated machine for {{ ocs_worker_node }} for deletion
        shell: "oc annotate machine/$(oc get bmh/{{ ocp_cluster_name}}-{{ ocs_worker_node }} -n openshift-machine-api --no-headers -o custom-columns=blah:.spec.consumerRef.name) machine.openshift.io/cluster-api-delete-machine=yes -n openshift-machine-api"
        register: annotate_ocs_node
        failed_when: annotate_ocs_node.stderr != "" and "not found" not in annotate_ocs_node.stderr
        environment:
          PATH: "{{ oc_env_path }}"
          KUBECONFIG: "{{ kubeconfig }}"

      - name: Scale worker machineset down by 1
        shell: "oc scale machineset/{{ ocp_cluster_name }}-worker-0 -n openshift-machine-api --replicas=$(($(oc get machineset/{{ ocp_cluster_name }}-worker-0 -n openshift-machine-api --no-headers -o custom-columns=blah:.status.replicas) - 1))"
        environment:
          PATH: "{{ oc_env_path }}"
          KUBECONFIG: "{{ kubeconfig }}"

      - name: Wait for {{ ocs_worker_node }} to be de-provisioned
        shell: oc get nodes -l node-role.kubernetes.io/worker
        retries: 100
        delay: 15
        register: storage_node_removed
        until: ocs_worker_node not in storage_node_removed.stdout
        environment:
          PATH: "{{ oc_env_path }}"
          KUBECONFIG: "{{ kubeconfig }}"

      - name: Scale worker machineset up by 1
        shell: "oc scale machineset/{{ ocp_cluster_name }}-worker-0 -n openshift-machine-api --replicas=$(($(oc get machineset/{{ ocp_cluster_name }}-worker-0 -n openshift-machine-api --no-headers -o custom-columns=blah:.status.replicas) + 1))"
        environment:
          PATH: "{{ oc_env_path }}"
          KUBECONFIG: "{{ kubeconfig }}"

      - name: Wait for {{ ocs_worker_node }} to be provisioned
        shell: oc get nodes -l node-role.kubernetes.io/worker
        retries: 100
        delay: 15
        register: storage_node_ready
        until: "(storage_node_ready.stdout | regex_findall(ocs_worker_node + '   Ready') | length) == 1"
        environment:
          PATH: "{{ oc_env_path }}"
          KUBECONFIG: "{{ kubeconfig }}"
        
    - name: Create OCS YAMLs working dir
      file:
        path: "{{ working_yamls_dir }}/ocs"
        state: directory
        mode: 0755

    - name: Write OCS YAMLs to working dir
      template:
        src: ocs/{{ item }}.yaml.j2
        dest: "{{ working_yamls_dir }}/ocs/{{ item }}.yaml"
      with_items:
        - local-storage-sub
        - local-storage-volumes
        - ocs-storage-cluster
        - ocs-sub

    - name: Label {{ ocs_worker_node }} as OCS-capable
      shell: |
          oc label node/{{ ocs_worker_node }} cluster.ocs.openshift.io/openshift-storage='' --overwrite=true;
          oc label node/{{ ocs_worker_node }} topology.rook.io/rack=rack0 --overwrite=true
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"
    
    - name: Deploy local-storage subscription
      shell: |
        oc new-project local-storage
        oc annotate project local-storage --overwrite openshift.io/node-selector=''
        oc apply -f {{ working_yamls_dir }}/ocs/local-storage-sub.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for local-storage operator to be ready
      shell: oc get pods -n local-storage
      retries: 100
      delay: 20
      register: local_storage_operator_ready
      until: (local_storage_operator_ready.stdout | regex_findall('local-storage-operator-.+-.+Running') | length) == 1
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Create local-storage PVs
      shell: |
        oc apply -f {{ working_yamls_dir }}/ocs/local-storage-volumes.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for local-storage PVs to be ready
      shell: oc get pv -n local-storage
      retries: 100
      delay: 15
      register: local_storage_volumes_ready
      until: (local_storage_volumes_ready.stdout | regex_findall('local-pv-.+(Available|Bound)') | length) == (local_storage_volumes_ready.stdout | regex_findall('local-pv-.+') | length) and (local_storage_volumes_ready.stdout | regex_findall('local-pv-.+(Available|Bound)') | length) != 0
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Deploy OCS subscription
      shell: oc apply -f {{ working_yamls_dir }}/ocs/ocs-sub.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for OCS, rook-ceph and noobaa operators to be ready
      shell: oc get pods -n openshift-storage
      retries: 100
      delay: 20
      register: ocs_operators_ready
      until: (ocs_operators_ready.stdout | regex_findall('ocs-operator-.+-.+Running') | length) == 1 and (ocs_operators_ready.stdout | regex_findall('rook-ceph-operator-.+-.+Running') | length) == 1 and (ocs_operators_ready.stdout | regex_findall('noobaa-operator-.+-.+Running') | length) == 1
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Deploy OCS cluster
      shell: oc apply -f {{ working_yamls_dir }}/ocs/ocs-storage-cluster.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    # FIXME: We currently check for the pods enumerated in the "until" clause below due to the behavior of the 
    #        StorageCluster resource's "phase" output.  Unfortunately the phase is reported as "Ready" as soon
    #        as the StorageCluster resource appears, which is not actually true, as the various OCS pods required
    #        for the proper functioning of the underlying Ceph cluster have not yet been created.  The phase 
    #        eventually transitions to "Progressing" and then to "Ready" again at a later time.  Thus, we can't
    #        rely on this state and have to check for the individual pods for now.
    - name: Wait for OCS pods to be ready
      shell: oc get pods -n openshift-storage
      retries: 100
      delay: 30
      register: ocs_pods_ready
      until: ((ocs_pods_ready.stdout | regex_findall('csi-cephfsplugin-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('csi-cephfsplugin-.+') | length) and (ocs_pods_ready.stdout | regex_findall('csi-cephfsplugin-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('csi-rbdplugin-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('csi-rbdplugin-.+') | length) and (ocs_pods_ready.stdout | regex_findall('csi-rbdplugin-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('rook-ceph-mon-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('rook-ceph-mon-.+') | length) and (ocs_pods_ready.stdout | regex_findall('rook-ceph-mon-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('rook-ceph-osd-\d-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('rook-ceph-osd-\d-.+') | length) and (ocs_pods_ready.stdout | regex_findall('rook-ceph-osd-\d-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('noobaa-core-0.+Running') | length) == 1)
             and ((ocs_pods_ready.stdout | regex_findall('noobaa-db-0.+Running') | length) == 1)
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Enable Ceph tools pod
      shell: "oc patch OCSInitialization ocsinit -n openshift-storage --type json --patch '[{ \"op\": \"replace\", \"path\": \"/spec/enableCephTools\", \"value\": true }]'"
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - debug:
        msg:
          - "You can now check the health of your OCS cluster via:"
          - "oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)"
          - "ceph -s"
